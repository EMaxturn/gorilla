# Data Generation Pipeline Implementation Details

## Single-turn Dataset

### Data Collection

For the *single-turn* tasks, we divide the data collection into two categories based on their evaluation method: **AST** categories that use Abstract Syntax Tree, and **Execute** categories that evaluate by execution. The evaluation methodology is discussed in Sections [AST Substring Matching](#) and [Execution Matching](#).

- **AST**:  
  We collect functions from popular GitHub repositories (top 100 starred) in Python, Java, and JavaScript. These functions are well-documented, making them ideal candidates for our downstream tasks. We exclude trivial functions such as `__init__`, `__eq__`, and functions with fewer than two parameters (excluding the `self` parameter) to ensure complexity and relevance.

- **Execute**:  
  This category is divided into two sub-categories based on the backend type:  
  1. **Pure Python Functions**: Manually constructed functions inspired by common math and physics calculations. These are purely executable Python functions that don't rely on external APIs.  
  2. **Python Functions Wrapped APIs**: Functions that invoke API calls from popular public API providers such as ExchangeRate API, OMDb API, and Geocoding API. We focused on GET requests, which are the most common in real-world scenarios. These functions demonstrate the model's ability to generate executable REST API calls using `requests.get()` with hardcoded URLs and a clear description of the function’s purpose and parameters.

### Data Preprocessing

We pre-process the data to extract useful context for downstream data generation tasks.

- **AST**:  
  We extract function names, descriptions, parameter names, types, and default values directly from function signatures and docstrings.

- **Execute**:  
  For executable functions, we use Python's `requests.get()` as a function document template. The schemas include base URLs, query parameters, path parameters, and body parameters.


### Data Generation

We transform the extracted function information and context—such as Python docstrings and API documentation from sources like RapidAPI's ExchangeRate API—into well-formatted function documents JSONs, which are generated by prompting GPT-4o-0806. This ensures consistent formatting, including descriptions of function, parameters, types, and default values, making them compatible with our downstream evaluation pipeline.

Once the function documentation is generated, we create realistic user questions by prompting GPT-4o-0806 based on these documents and their use in the original codebase or API context. These questions are evaluation prompts for the single-turn function calling tasks.

### Data Transformation

To introduce complexity and mimic diverse real-world function-calling scenarios, we expand the dataset through various transformations (detailed in Data Augmentation). These include simulating different function calling patterns (e.g., parallel and multiple calls) and introducing scenarios with incomplete or missing information to test model behavior. These transformations are done by either 1) prompting GPT-4o-0806 based on the original function documentations and previously generated user questions, which creates new generated function documentations and user questions, for example parallel category is created by generating user questions that require parallel function calling using the same simple category function documents, or 2) reorganizing entries from existing test categories. For example, function irrelevance test entries are created by swapping the function documentation of simple category with the an irrelevant function documentation from another simple category entry. 

### Data Validation

We ensure:
1. Function documentation adheres to the **FCL** format, including all required function schema fields.
2. Function parameters are precisely defined and correctly categorized.
3. User prompts are relevant, clear, and properly aligned with the corresponding function documentation.

We involved three human experts for this validation process.

---

## Crowd-sourced Dataset

### Data Collection

For the *live* dataset, 64,517 real-world user queries were collected between 2024-02-26 and 2024-04-01 via our hosted model endpoint.

### Data Preprocessing

- **Deduplication**:
  We applied ROUGE-L scoring and OpenAI’s text-embedding models to remove duplicate queries and function docs. Specifically, we first calculate pairwise ROUGE-L scores between all function documents and group the duplicate function documents that have a score greater than 0.8. Then, we use OpenAI's text-embedding-3-small model to calculate pairwise cosine similarity scores between all user queries and remove the duplicate user queries that have a score greater than 0.8. Note that both 0.8 scores are chosen empirically.

- **Exclusion of Public Datasets**:  
  Queries from public test sets (e.g., Nexus Function Calling Leaderboard) were filtered out to prevent contamination.

- **Data Parsing**:  
  Valid function documentation was parsed into a JSON format compatible with the **FCL** evaluation pipeline.

### Data Generation

The expert-curated dataset does not involve a separate data generation phase. All entries are authentic user-contributed data and directly proceed to transformation.

### Data Transformation

- **Minimum Edit Transformation**:  
  Human annotators (4 trained undergraduates) carefully review and apply minimal yet necessary edits to standardize the data while preserving its original intent, the following are the guidelines for the edits:

  For function documentation:
  - Ensure all required fields are present (name, description, parameters)
  - Verify parameter definitions are precise and correctly typed (boolean, array, string, etc.)
  - Add missing required/default value specifications
  - Standardize dictionary parameters to include "property" field details
  - Validate array parameters have proper "items" field type specifications
  - Maintain strict compliance with BFCL format requirements

  For user prompts:
  - Preserve original semantic meaning and format
  - Enhance clarity by eliminating ambiguity
  - Replace sensitive information with appropriate placeholders
  - Verify relevance to associated function documentation
  - Ensure concrete parameter values are clearly presented
  - For multi-role prompts (system/user/assistant), only edit user portions

  All modifications follow a "minimum edits" principle - changes are made only when necessary for pipeline compatibility or clarity, while maintaining the original functionality and intent of both documentation and prompts.

### Data Validation

In addition to the single-turn validation steps, we also ensure:
1. Transformed function documentation and prompts preserve their original semantic meaning.
2. Sensitive information in user prompts is replaced with placeholders, and ambiguous content is clarified.

---

## Multi-turn Dataset

### Data Collection

The multi-turn dataset was built using a custom API codebase spanning eight domains: Vehicle Control, Trading Bots, Travel Booking, File System, Messaging, Twitter, Ticket Booking, and Math. Each API simulates real-world multi-turn function calls. Here, we call Vehicle Control, Trading Bots, Travel Booking, File System as "primary" APIs, and Messaging, Twitter, Ticket Booking as "companion" APIs.

The four primary API domains are evenly distributed across the test cases in Base, and Augmented Multi-Turn. For example, there are 200 test entries in Base category and 0-49 utilizes Gorilla File System, 50-99 utilizes Vehicle Control, 100-149 utilizes Trading Bots, and 150-199 utilizes Travel Booking.


### Data Preprocessing

Once the API codebase is established, we (5 trained undergraduates) manually construct an execution (or dependency) graph where each function represents a node. We manually map out direct edges, meaning a function's output is an input of the downstream function. This graph allows us to model cross-API behaviors, simulating realistic multi-turn function calls across different domains. Whenever we need a dataset, we sample a node on the graph, and randomly traverse through the graph to generate an execution path. Through the execution path, we are able to extrapolate the scenario that will be presented to the LLMs. For example, one of the edges can be from `get_stock_info` to `place_order`, since the stock name or price of the stock is the input of the `place_order` function. 

![API Execution Graph](./function-call-leaderboard/assets/multi-turn-graph-edges.png)


### Data Generation

The data generation process for multi-turn interactions included:

- **Task Generation**:  
  We prompted GPT-4o-0806a to invoke a series of function calls and derive a natural language query requiring that function trajectory. The questions vary in tone and style to simulate different user personas and interaction styles. We adopted personas from [Persona Hub](https://arxiv.org/abs/2402.08668), including:
  - High school physics teachers  
  - Science historians  
  - Elderly hermits  

- **Function Lists**:  
  For each task, a list of available functions from both primary and companion APIs was provided. This was randomly sampled from the primary and companion APIs that follows the API dependency graph paths.

- **Initial Configurations**:  
  Initial states (e.g., pre-authenticated sessions) were set to avoid redundant interactions and focus on meaningful tasks.

- **Human-labeled Ground Truth**:  
  Expert human labelers reviewed and labeled each multi-turn interaction with ground truth.

### Data Transformation

For miss_func, miss_params, we start from the same 200 base entries. We "augment" these entries by manually (5 trained undergraduates) to identify one turn to get rid of one informoation. For miss_func, we remove one tool in one of the turns, for example, when using the FileSystem API, we remove the `ls` function call, and remove the LLM's capability to use `ls` function call and see directory structure. In this case, we are testing the LLM's ability to identify the missing function call and ask for clarification. For miss_params, we manually remove one parameter information in one turn of the conversation, modify the user's query, and see if the LLM can identify the missing parameter and ask for clarification.

### Data Validation

Validation involved:

- **Question Validation**: Questions were checked for specificity and completeness.
- **Ground Truth Validation**: Multi-turn function call sequences were verified against ground truth.
- **Initial Configuration Validation**: Ensured completeness and relevance of initial states.
- **Function List Validation**: Verified that all necessary functions were included.
- **API Code Validation**: Used unit tests and format checkers to ensure code compliance and consistency.
